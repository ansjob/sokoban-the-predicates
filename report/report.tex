\documentclass[a4paper,11pt]{article}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{a4wide}
\usepackage{numprint}
\usepackage{url}
\usepackage{moreverb}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{cite}
\pagestyle{fancy}

\newcommand{\tab}{\hspace*{2em}}
\title{A reverse and backwards Sokoban solver \\ }
\author{Andreas Sjöberg \\ \url{ansjob@kth.se} 
		\and Andreas Gabrielsson \\ \url{andregus@kth.se} 
		\and Marcus Larsson \\ \url{marcular@kth.se}
	}

\fancypagestyle{plain}
{
	\fancyhf{}
	\renewcommand{\headrulewidth}{0pt}
	\fancyfoot[C]{}
}

\begin{document}

\thispagestyle{plain}
\maketitle

\begin{abstract}
This report describes the implementation of a Sokoban puzzle game solver,
for the course in artificial intelligence.
The Sokoban puzzle is a popular AI research area, as it proves to be quite
the difficult problem to solve.
Our implementation includes bi-directional best-first-searches with various
optimizations for avoiding deadlocks, limiting memory usage and maximizing
efficiency.
The report concludes that while decent results were obtained with our implementation,
many further improvements can be done to solve the more difficult puzzles for this game.
\end{abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}
This paper is a report describing the Sokoban puzzle solver we have implemented for the course
DD2380 Artificial Intelligence in the fall of 2012.

\section{Problem statement}

In the game of Sokoban, the player takes the role of a warehouse keeper in a
(usually maze-like) warehouse with boxes in it.
The warehouse also contains certain positions (goal positions) where the boxes \emph{should} be located.
The objective of the game is to push each of the boxes so that all boxes end up
in a goal position.
To push a box, the player has to stand next to the box,
and the square onto which the box is pushed has to be empty.

In other words, the problem for us to solve is implementing an agent that,
given a Sokoban board, finds the sequence of moves for the player so that
it solves the puzzle.
The movements can be one of the directions \verb!up!, \verb!down!, \verb!left!, or \verb!right!.

The boards are represented using a standard Sokoban format. See this link for details:
\url{http://sokobano.de/wiki/index.php?title=Level_format}.

\section{Method}

The first observation we made was that we could divide the problem into two sub-problems
to simplify our implementation.
First we need to find the (valid) sequence of pushed boxes that are required to solve
the puzzle, and after that we must solve the, much simpler, problem of moving the player
between the various boxes that should be pushed.
Given a sequence of two boxes that should be pushed (basically two positions in a maze),
finding the moves in between these push-actions is trivial.

With this breakdown of the problem, the remaining problem is to find the
sequence of boxes pushed.
To solve this, we have implemented a search in the state space that a given puzzle defines.
When we find a path between the starting state and the solved state (where each box is in a goal position),
we simply find the transitions between all of these states and concatenate these to get a complete solution.

\subsection{State definition}
\ref{sec:statedef}

So what do we mean by a state in Sokoban?
The naive definition of a state could include the whole puzzle,
including the positions of the boxes, the player.
Including walls and goal positions in the state would be wasteful,
as they can never be moved throughout the game.

In our implementation, a state simply consists of the set of coordinates where the boxes are located,
and the set of coordinates next to a box that the player can currently reach.
With this definition, the player is able to move freely in the
open area where it currently is, without entering a different state.
Yet it still captures the difference between two states where the player is on a different side of a
box it is unable to move around.
For example, look at the three maps in Figure \ref{example:states}.
The first two represent the same state,
while the last one represents a different state.
The reachable box neighbours are marked with an \verb!R!.

\begin{figure}[h!]
	\centering
\begin{center}
\begin{verbatimtab}[6]
			############ ############ ############
			#   R$    .# #   R$    .# #    $R   .#
			#   R$    .# #  @R$    .# #    $R@  .#
			#@  R$    .# #   R$    .# #    $R   .#
			############ ############ ############
\end{verbatimtab}
\end{center}
	\label{example:states}
	\caption{
		Three different situations that yield two different Sokoban states.
	}
\end{figure}

Furthermore, by using sets of coordinates rather than matrices with the boxes in,
we save some space, since the number of boxes will always be less than the number of squares in any
given puzzle.
The coordinate system we have used in our implementation is the same as in computer graphics.
That is, the top left corner is defined to be the origin, and a coordinate is defined to be
$(row, col)$.

\subsubsection{Example}
Take a look at the board below for example.
Here the state is
$( \{(2, 1), (2, 4), (3, 4)\}, \{(1,1), (2,2), (2,3), (2,5), (3,1), (3,3), (3,5)\})$,
where the box locations are
$\{(2, 1), (2, 4), (3, 4)\}$,
and the reachable box neighbours would be
$\{(1,1), (2,2), (2,3), (2,5), (3,1), (3,3), (3,5)\} $.

\begin{verbatimtab}
 0123456
0#######
1#.@ # #
2#$* $ #
3#   $ #
4# ..  #
5#  *  #
6#######
\end{verbatimtab}

\subsection{Finding a transition between states}

A transition between two states consists of walking up to a box and pushing it.
An inherent problem with the way our states are defined is that it does not include the location
of the player, so the moving up to a box part becomes impossible to solve because
we do not know from where to start moving.
To remedy this, we simply include the location of the player in the state,
but do \emph{not} consider it when comparing states.

With this information, the problem is very solvable.
The first part is accomplished with a flooding-algorithm that
determines the walking distance to every reachable coordinate in a breadth
first manner until it finds the target, and then follows the trail back to
where the player stood in the last transition.
The second part (pushing the box) is straight forward, it is only a matter 
of finding the direction of the box to push relative the push-from position.

\subsection{Finding a winning path}

There are two major approaches to solving Sokoban puzzles; forwards and backwards solvers.
The forwards solvers simply starts in the state the board is in and expands a search node
by pushing a box and moving the player.
The backwards algorithm starts in each possible end state of the game,
and expands a search node by \emph{pulling} a box.
The possible end states of the game are simple to find;
they consist of the states where all boxes are on a goal, and the player
is standing next to a box.

In our implementation, we decided to use both methods.
The reason for this is because some puzzles are designed to be difficult
for forward-solvers, and some are designed to be difficult for backwards-solvers.
This way, we get the benefits that each method offer.
Also, the usual benefits of bidirectional search makes this approach compelling.

To implement the bidirectional search, we have the two different algorithms
for forward and backwards searching, and they alternate in execution.
If one of them detects that they are visiting a state the other has already visited,
a solution has been found, and the paths are ''stitched together'' to form the answer.

\subsubsection{Prioritizing states}
\label{sec:prio}

Since the state-space grows rapidly, there is a need to
prioritize the order in which the states are explored.
We decided to use a best-first-search approach for both directions of the search.
Given a good evaluation function, best-first-search is a reasonable
approach, since it will explore states close to the solution first.
This is to prefer compared to for example an $A*$ algorithm,
because we are not concerned with how deep in the
search tree we have explored, we only need to find 
\emph{a} solution as quick as possible.

We implemented three evaluation functions that given a state return an integer value in the range
$[0, 99]$, where higher is better.
One function is a mobility value, one is a value designed to make
it more prioritized to move boxes close to the last box moved,
and the last one is the percentage of boxes on a goal.
We considered having a distance function that would get a ''better score''
when boxes are moving closer to a goal, but this was not very efficient
because it did not take into account the fact that the agent could push several boxes
close to the same goal to get a good score.
This may be due to any of the following reasons;
	we computed the Manhattan distance between a box and a goal,
	we ignored any walls in between the box and the goal,
	and we computed the distance between a box and \emph{the closest} goal, which may not be correct.


\subsection{Changing heuristic weights}
We have continuously monitored our agent using VisualVM %KÄLLA?
. By doing that we could see that the memory is filled up after certain amount of time, around 30 seconds. 
Then the agent are using the CPU until the memory is filled up and after that the garbage collector tries to free 
memory but are unable to do so because almost every object are referenced by the agent. This results in a behavior 
where both the agent and the garbage collector are unable to do any valuable work so it does not come closer to the solution. 

This is of course a bad usage of the time frame. To solve this problem the agent tries to find a solution using the hard coded
weights on the heuristics. If it fails within a time limit it starts over with the continuity value weighted more because this could help 
find a solution for maps where there are a lot of tunnels. If this also fails within a time limit the agent will start to randomize 
the weights and try a lot of different weights for a short time. 

The problem with this solution is that all previous work becomes useless since the agent starts over again. But because of the memory
problem it would not have done any useful work anyway so it is best to start over again and hope that the new approach is better. 

\subsection{Eliminating dead states}

\label{sec:dead}

We implemented the simplest type of dead state detection we could think of,
namely to avoid pushing boxes into static dead positions in the puzzle.
By static dead positions we mean positions where if the map was empty,
we could not move a box in that position into any of the goals.

The algorithm for this is simple, and runs one time before the search starts.

\clearpage
\begin{algorithm}
\caption{Marking Sokoban board positions as static dead or alive}
\label{alg:dead_positions}
\begin{algorithmic}
	\Function{DeadPositions}{$board$}
		\For {$position \in board$}
			\State $position \gets dead$
		\EndFor
		\For {$goal \in board.goals$}
			\For {$position \in board$}
				\If {\Call{CanPullFromTo}{$goal$, $position$}}
					\State $position \gets alive$
				\EndIf
			\EndFor
		\EndFor
	\EndFunction
\end{algorithmic}
\end{algorithm}

Algorithm \ref{alg:dead_positions} will mark corners and positions along the
''outer walls'' as statically dead, unless they contain a goal or a path to a goal.
In the example below, the dead positions have been marked with \verb!X!.
There are of course significantly more dead situations but this simple approach 
allows us to prune lots of dead states with little effort.

\begin{verbatimtab}
 0123456
0#######
1#. X#X#
2# .  X#
3#    X#
4# .. X#
5#X . X#
6#######

\end{verbatimtab}

The other dead states are not as static as the ones discussed above. 
It can for example be when four boxes are pushed together in a two times two square when they are not on a goal.
In the backwards solver this particular kind of dead state is not really a problem since it is
impossible to pull four boxes to such a square.
In the forward solver this problem is tackled by using the mobility value discussed in section~\ref{sec:prio}. 

There are even more dead states that are harder to detect but those are not considered simply because it is not very simple.

\section{Results}


\subsection{Performance Optimizations}
\label{sec:opt}
In addition to the algorithm optimizations, such as the state definition and bidirectional search,
we have made some implementation specific optimizations.
In order to squeeze some better performance we implemented
the coordinates using the multiton pattern.
This means there will not be several object instances for the same coordinate,
but only references to the same one.
This saves lots of memory, since many states will have references to the same coordinates.

When sampling the agent with VisualVm we noticed that a lot of time was spent in the Sokoban states \verb!equals()! and \verb!hashCode()! methods. 
Typical values were 40\% of the time in each method which would mean that only 20\% of the time was used to other methods that actually brings
the agent closer to the solution.
That is no surprise since all states are stored in hash tables. By only calculating the hash value once and then store that value
we were able to reduce the time spent in \verb!hashCode()! to around 20\% of the total time. \verb!equals()! was improved as well
down to using around 10\% of the total time. This means that 70\% of the time is used for other purposes.


\subsection{First approach}
Our first approach, by the milestone deadline, did not use almost any of the optimizations discussed in the method description and in section~\ref{sec:opt}.
By then the only heuristics value was the goal on boxes value and only all maps were solved by the forward solver. 
The static deadlock detection discussed in section~\ref{sec:dead} was the only one.
The strong features of this implementation was the smart state definition discussed in~\ref{sec:statedef} and the deadlock detection. 

By then the agent solved 38 maps on the evaluation server, 42 maps on the test server with port 5032 and none of the first 100 maps on the 
test server with port 5031. 

Even before that, when regular breath-first-search was used, the agent solved 16 maps on the evaluation server. 
Without the deadlock detection the agent were not able to solve any of the maps on the test server with port 5032.

\subsection{Final approach}

In the final agent all optimization discussed previously are present. 

\section{Discussion}

\section{Conclusions}


\end{document}
